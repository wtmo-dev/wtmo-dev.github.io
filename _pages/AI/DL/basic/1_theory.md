---
title: "1. Theory of deep learning"
tags:
    - deep learning
    - theory
date: "2023-10-30"
thumbnail: "/assets/img/thumbnail/logo.jpg"
---

# 인공신경망
---
* 퍼셉트론(Perceptron)  
TLU(Threshold Logic Unit)이라고도 불리며 입력들에 대하여 출력을 구하는 유닛이다. 출력은 활성화 함수에 의하여 발생이 되며 heavyside, sgn과 같은 함수가 활용된다.  
* 다중 퍼셉트론  
하나의 층으로 구성이된 퍼셉트론은 XOR과 같은 문제를 해결할 수 없고 이를 해결하기 위해 2개 이상의 층으로 구성이된 퍼셉트론을 만드는것이다.
* 역전파(backpropagation)  
다중 퍼셉트론의 학습법으로 효율적인 계산법은 경사 하강법을 사용할 수 있다.  
역전파 기법의 서순은 아래와 같이 작용한다.  
    1. 미니배치들로 1epoch가 될때까지 반복
    2. 각 미니배치들을 순차적으로 1개의 레이어씩 진행함
    3. 출력층에 도달(정방향 계산)
    4. 오차 측정
    5. 순처적으로 되돌아가며 가중치를 계산
    6. 경사 하강법으로 가중치를 변경
* 활성함수의 필요성  
활성함수가 없으면 모든 층이 선형성을 가지기 때문에 비선형성을 가지는 복잡한 문제를 해결 할 수 없다.   


# 심층신경망(DNN)
---
은닉층을 2개 이상 가지는 신경망(다중 퍼셉트론)

* 선형회귀
가중치, 편향을 가지고 단일 선형 또는 다중 선형 회귀 분석을 만들 수 있다.  
* 가설 설정
제공 되는 데이터를 통하여 관계를 유추해 수식으로 예측해보는것을 가설이라 한다.
* 손실 함수
일반적으로 MSE와 같은 방법을 사용
* 경사 하강법
손실 함수를 최소화하기 위해 편미분을 취해 찾아가는 방법
* epoch
순전파, 역전파를 1회 끝까지 완료하는 횟수
* batch size
전체의 데이터에서 매개변수 업데이트에 사용할 데이터들의 집합
* iterator
1 epoch를 위해 필요한 batch의 수

# 순환신경망(RNN)
---
Recurrent Neural Network로 순차적(sequential) 학습의 한 종류이다.  
과거의 학습에 Weight를 가하여 현재 학습에 반영하는 신경망
* 단점
중요한 정보가 오래될 경우 영향력을 크게 상실할 수 있다.  
이를 보완하기 위해 LSTM(Long Short-Term Memory)와 간소화된 GRU(Gated Recurrent Unit)가 있다.

# 합성신경망(CNN)
---
Convolutional Neural Network로 이미지 처리에 효과적인 학습법이다.  
아래의 두가지 구조로 구성된다.  
* Convolution Layer  
kernel/filter를 이용해서 데이터를 특성 맵(feature map)으로 만드는것  
    * 스트라이드(stride)  
kernel/filter가 특성 맵을 만들때 이동하는 거리를 의미한다.  
    * 패딩(padding)  
특성맵의 크기 축소를 해소하는 방법으로 빈공간으로 채우는 방법  
* Polling Layer  
특성 맵을 풀링 연산(max, average, ...)을 통하여 다운 샘플링하는 것

# 그래프신경망(GNN)
---
Graph Neural Network로 그래프 데이터를 활용하는 신경망이다.  
비정형의 복잡한 형태로 인하여 분석이 힘들다.  

# 가중치 초기화
---
Weight initialization은 학습 초기 가중치를 결정하는 방법으로 효율적이지 못한값을 제공할 경우 미분값의 소실, 발산이 생길 수 있다.  
아래의 초기화 기법들은 이와 같은 문제를 효율적으로 다룰 수 있다.  
* He  
ReLU와 같은 함수를 사용  
    * ReLU 함수는 양수값에서 수렴하지 않으며 속도도 빠르지만 훈련도중 가중치가 음수를 가지게 되면 기울기가 0이 되는 Dying ReLU이슈가 있다.  
    * LeakyReLU는 위의 이슈를 해결하기 위하여 음수도 기울기를 가하는 방법이다. $LeakyReLU(z) = max(\alpha z, z)$
    * RReLU는 정해진 범위의 $\alpha$을 랜덤으로 선택하는것

    * PReLU는 $\alpha$가 학습하면서 결정이되며 데이터가 적으면 과적합 위험이 있다.
    * ELU는 $z < 0$에서 $\alpha (e^z-1)$을 가지게되며 계산이 느리지만 학습력이 좋다.
* 글로럿  
tanh, logistic, softmax와 같은 함수 또는 활성화 함수 사용 X  
인풋과 아웃풋 노드 수의 평균에 의존적이다.  
* 르쿤  
SELU와 같은 함수를 사용  
인풋의 노드 수에 의존적이다.  

# 배치 정규화
---
가중치 초기화는 초기의 가중치를 통하여 이후의 안정성을 제공하지만 완전히 안정적이지는 않다. 그렇기 때문에 배치 정규화를 이용해서 은닉층 내부에서 안정화를 제공할 수 있다.  
* 활성화 함수 전후에 연산을 추가하여 진행되며 배치 정규화를 진행하기 위해서는 입력의 평균과 표준 편차를 필요로 한다.  
* 각 층의 입력의 값을 활용하기 때문에 테스트를 할때는 전체 훈련결과를 이용하여 정규화를 진행한다.  

# 그래디언트 클리핑
---
그래디언트 폭주가 발생하지 않게 역전파에서 임계값을 초과하지 않게한다.  

# 전이학습
---
transfer learning이라고도 하며 이전에 학습한 층을 재활용하는 것이다. 상위층의 경우 재사용시 다른 모델에 최적화 되어 있기때문에 하위층을 주로 활용한다. 
